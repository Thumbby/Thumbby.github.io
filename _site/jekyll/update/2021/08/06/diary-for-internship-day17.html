<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>实习日记17(消息队列+ES搜索引擎)</title>
  <meta name="description" content="Day17">
  <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:600' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="//css/main.css">
  <link rel="canonical" href="http://Thumbby.github.io//jekyll/update/2021/08/06/diary-for-internship-day17.html">
  <link rel="alternate" type="application/rss+xml" title="Thumbby's Blog" href="http://Thumbby.github.io//feed.xml" />
</head>

<style>
  table{
    border-left:1px solid #000000;border-top:1px solid #000000;
    width: 100%;
    word-wrap:break-word; word-break:break-all;
  }
  table th{
  text-align:center;
  }
  table th,td{
    border-right:1px solid #000000;border-bottom:1px solid #000000;
  }
</style>

  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="//">Thumbby's Blog</a>

    <nav class="site-nav">
        
          
          <a class="page-link" href="//about/">About</a>
          
        
          
        
          
        
          
        
    </nav>

  </div>

</header>


    <div class="page-content">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">实习日记17(消息队列+ES搜索引擎)</h1>
    <p class="post-meta">Aug 6, 2021</p>
  </header>

  <article class="post-content">
    <h2 id="day17">Day17</h2>

<ul>
  <li>
    <p><strong>Java学习，基于https://github.com/doocs/advanced-java</strong></p>
  </li>
  <li>
    <p>如何解决消息队列的延时以及过期失效问题。</p>

    <ul>
      <li>
        <p>例：消息队列满了之后怎么处理？有几百万的消息持续挤压几个小时怎么解决？</p>
      </li>
      <li>
        <p>可能导致的原因：消费端出了问题，不消费了或者消费的速度及其慢。即消费端出现故障，大量消息在MQ中挤压。下面举几个常见事故和解决方案。</p>

        <ul>
          <li>
            <p>大量消息在MQ中积压了几个小时没有解决</p>

            <p>可以修复consumer问题让其恢复消费速度，再等其消费完毕，不过这个方法太傻，耗时长。</p>

            <p>或者可以尝试紧急扩容，具体操作步骤和思路如下：</p>

            <ul>
              <li>先修复 consumer 的问题，确保其恢复消费速度，然后将现有 consumer 都停掉。</li>
            </ul>
          </li>
          <li>新建一个 topic，partition 是原来的 10 倍，临时建立好原先 10 倍的 queue 数量。
            <ul>
              <li>然后写一个临时的分发数据的 consumer 程序，这个程序部署上去消费积压的数据，消费之后不做耗时的处理，直接均匀轮询写入临时建立好的 10 倍数量的 queue。</li>
            </ul>
          </li>
          <li>接着临时征用 10 倍的机器来部署 consumer，每一批 consumer 消费一个临时 queue 的数据。这种做法相当于是临时将 queue 资源和 consumer 资源扩大 10 倍，以正常的 10 倍速度来消费数据。
            <ul>
              <li>等快速消费完积压数据之后，得恢复原先部署的架构，重新用原先的 consumer 机器来消费消息。</li>
            </ul>
          </li>
          <li>
            <p>MQ中消息过期失效了</p>

            <p>假设使用的是 RabbitMQ，RabbtiMQ 是可以设置过期时间的，也就是 TTL。如果消息在 queue 中积压超过一定的时间就会被 RabbitMQ 给清理掉，这个数据就没了。这就不是说数据会大量积压在 mq 里，而是大量的数据会直接搞丢。</p>

            <p>这个情况下，就不是说要增加 consumer 消费积压的消息，因为实际上没啥积压，而是丢了大量的消息。我们可以采取一个方案，就是批量重导。就是大量积压的时候，直接丢弃数据了，然后等过了高峰期以后。这个时候开始写程序，将丢失的那批数据，写个临时程序，一点一点的查出来，然后重新灌入 mq 里面去，把白天丢的数据给他补回来。也只能是这样了。</p>

            <p>假设 1 万个订单积压在 mq 里面，没有处理，其中 1000 个订单都丢了，只能手动写程序把那 1000 个订单给查出来，手动发到 mq 里去再补一次。</p>
          </li>
          <li>
            <p>MQ快写满了</p>

            <p>如果消息积压在 mq 里，你很长时间都没有处理掉，此时导致 mq 都快写满了，咋办？这个还有别的办法吗？没有，谁让你第一个方案执行的太慢了，你临时写程序，接入数据来消费，消费一个丢弃一个，都不要了，快速消费掉所有的消息。然后走第二个方案(MQ中消息过期失效了)，等过了高峰期再补数据吧。</p>
          </li>
        </ul>
      </li>
      <li>
        <p>对于RocketMQ，官方对于消息挤压问题给出了如下方案:</p>

        <ul>
          <li>
            <p>提高消费并行度</p>

            <p>绝大部分消息消费行为都属于 IO 密集型，即可能是操作数据库，或者调用 RPC，这类消费行为的消费速度在于后端数据库或者外系统的吞吐量，通过增加消费并行度，可以提高总的消费吞吐量，但是并行度增加到一定程度，反而会下降。所以，应用必须要设置合理的并行度。 如下有几种修改消费并行度的方法：</p>

            <p>同一个 ConsumerGroup 下，通过增加 Consumer 实例数量来提高并行度（需要注意的是超过订阅队列数的 Consumer 实例无效）。可以通过加机器，或者在已有机器启动多个进程的方式。 提高单个 Consumer 的消费并行线程，通过修改参数 consumeThreadMin、consumeThreadMax 实现。</p>
          </li>
          <li>
            <p>批量方式消费</p>

            <p>某些业务流程如果支持批量方式消费，则可以很大程度上提高消费吞吐量，例如订单扣款类应用，一次处理一个订单耗时 1 s，一次处理 10 个订单可能也只耗时 2 s，这样即可大幅度提高消费的吞吐量，通过设置 consumer 的 consumeMessageBatchMaxSize 返个参数，默认是 1，即一次只消费一条消息，例如设置为 N，那么每次消费的消息数小于等于 N。</p>
          </li>
          <li>
            <p>跳过非重要消息</p>

            <p>发生消息堆积时，如果消费速度一直追不上发送速度，如果业务对数据要求不高的话，可以选择丢弃不重要的消息。例如，当某个队列的消息数堆积到 100000 条以上，则尝试丢弃部分或全部消息，这样就可以快速追上发送消息的速度。示例代码如下：</p>

            <div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">public</span> <span class="nc">ConsumeConcurrentlyStatus</span> <span class="nf">consumeMessage</span><span class="o">(</span>
            <span class="nc">List</span><span class="o">&lt;</span><span class="nc">MessageExt</span><span class="o">&gt;</span> <span class="n">msgs</span><span class="o">,</span>
            <span class="nc">ConsumeConcurrentlyContext</span> <span class="n">context</span><span class="o">)</span> <span class="o">{</span>
    <span class="kt">long</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">msgs</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="na">getQueueOffset</span><span class="o">();</span>
    <span class="nc">String</span> <span class="n">maxOffset</span> <span class="o">=</span>
            <span class="n">msgs</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="mi">0</span><span class="o">).</span><span class="na">getProperty</span><span class="o">(</span><span class="nc">Message</span><span class="o">.</span><span class="na">PROPERTY_MAX_OFFSET</span><span class="o">);</span>
    <span class="kt">long</span> <span class="n">diff</span> <span class="o">=</span> <span class="nc">Long</span><span class="o">.</span><span class="na">parseLong</span><span class="o">(</span><span class="n">maxOffset</span><span class="o">)</span> <span class="o">-</span> <span class="n">offset</span><span class="o">;</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">diff</span> <span class="o">&gt;</span> <span class="mi">100000</span><span class="o">)</span> <span class="o">{</span>
        <span class="c1">// TODO 消息堆积情况的特殊处理</span>
        <span class="k">return</span> <span class="nc">ConsumeConcurrentlyStatus</span><span class="o">.</span><span class="na">CONSUME_SUCCESS</span><span class="o">;</span>
    <span class="o">}</span>
    <span class="c1">// TODO 正常消费过程</span>
    <span class="k">return</span> <span class="nc">ConsumeConcurrentlyStatus</span><span class="o">.</span><span class="na">CONSUME_SUCCESS</span><span class="o">;</span>
<span class="o">}</span>
</code></pre></div>            </div>
          </li>
          <li>
            <p>优化每条消息消费</p>

            <p>举例如下，某条消息的消费过程如下：</p>

            <ul>
              <li>根据消息从 DB 查询【数据 1】</li>
              <li>根据消息从 DB 查询【数据 2】</li>
              <li>复杂的业务计算</li>
              <li>向 DB 插入【数据 3】</li>
              <li>向 DB 插入【数据 4】</li>
            </ul>

            <p>这条消息的消费过程中有 4 次与 DB 的 交互，如果按照每次 5ms 计算，那么总共耗时 20ms，假设业务计算耗时 5ms，那么总过耗时 25ms，所以如果能把 4 次 DB 交互优化为 2 次，那么总耗时就可以优化到 15ms，即总体性能提高了 40%。所以应用如果对时延敏感的话，可以把 DB 部署在 SSD 硬盘，相比于 SCSI 磁盘，前者的 RT 会小很多。</p>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>如果让你自己写一个消息队列，你会怎么设计架构</p>

    <ul>
      <li>这个问题一共考察如下两块：
        <ul>
          <li>你有没有对某一个消息队列做过较为深入的原理的了解，或者从整体了解把握住一个消息队列的架构原理。</li>
          <li>看看你的设计能力，给你一个常见的系统，就是消息队列系统，看看你能不能从全局把握一下整体架构设计，给出一些关键点出来。</li>
        </ul>
      </li>
      <li>可以从如下几个角度考虑：
        <ul>
          <li>首先这个 mq 得支持可伸缩性吧，就是需要的时候快速扩容，就可以增加吞吐量和容量，那怎么搞？设计个分布式的系统呗，参照一下 kafka 的设计理念，broker -&gt; topic -&gt; partition，每个 partition 放一个机器，就存一部分数据。如果现在资源不够了，简单啊，给 topic 增加 partition，然后做数据迁移，增加机器，不就可以存放更多数据，提供更高的吞吐量了？</li>
          <li>其次你得考虑一下这个 mq 的数据要不要落地磁盘吧？那肯定要了，落磁盘才能保证别进程挂了数据就丢了。那落磁盘的时候怎么落啊？顺序写，这样就没有磁盘随机读写的寻址开销，磁盘顺序读写的性能是很高的，这就是 kafka 的思路。</li>
          <li>其次你考虑一下你的 mq 的可用性啊？这个事儿，具体参考之前可用性那个环节讲解的 kafka 的高可用保障机制。多副本 -&gt; leader &amp; follower -&gt; broker 挂了重新选举 leader 即可对外服务。</li>
          <li>能不能支持数据 0 丢失啊？可以的，参考我们之前说的那个 kafka 数据零丢失方案。</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>ES( ElasticSearch)的分布式架构原理</p>

    <ul>
      <li>
        <p>ElasticSearch：基于lucene的分布式搜索引擎。</p>
      </li>
      <li>
        <p>基础架构设计：</p>

        <p>ES中存储数据的基本单位是索引，一个索引差不多是mysql中的一张表。一个 index 里可以有多个 type，每个 type 的字段都是差不多的，但是有一些略微的差别。假设有一个 index，是订单 index，里面专门是放订单数据的。所以就会在订单 index 里，建两个 type，一个是实物商品订单 type，一个是虚拟商品订单 type，这两个 type 大部分字段是一样的，少部分字段是不一样的。</p>

        <p>很多情况下，一个 index 里可能就一个 type，但是确实如果说是一个 index 里有多个 type 的情况（<strong>注意</strong>， <code class="language-plaintext highlighter-rouge">mapping types</code> 这个概念在 ElasticSearch 7. X 已被完全移除，详细说明可以参考<a href="https://github.com/elastic/elasticsearch/blob/6.5/docs/reference/mapping/removal_of_types.asciidoc">官方文档</a>），你可以认为 index 是一个类别的表，具体的每个 type 代表了 mysql 中的一个表。每个 type 有一个 mapping，如果你认为一个 type 是具体的一个表，index 就代表多个 type 同属于的一个类型，而 mapping 就是这个 type 的表结构定义，你在 mysql 中创建一个表，肯定是要定义表结构的，里面有哪些字段，每个字段是什么类型。实际上你往 index 里的一个 type 里面写的一条数据，叫做一条 document，一条 document 就代表了 mysql 中某个表里的一行，每个 document 有多个 field，每个 field 就代表了这个 document 中的一个字段的值。</p>

        <p><img src="https://github.com/doocs/advanced-java/raw/main/docs/high-concurrency/images/es-index-type-mapping-document-field.png" alt="es-index-type-mapping-document-field" /></p>

        <p>一个索引可以拆分成多个shard，每个 shard 存储部分数据。拆分多个 shard 是有好处的，一是<strong>支持横向扩展</strong>，比如你数据量是 3T，3 个 shard，每个 shard 就 1T 的数据，若现在数据量增加到 4T，怎么扩展，很简单，重新建一个有 4 个 shard 的索引，将数据导进去；二是<strong>提高性能</strong>，数据分布在多个 shard，即多台服务器上，所有的操作，都会在多台机器上并行分布式执行，提高了吞吐量和性能。</p>

        <p>接着就是这个shard的数据实际是有多个备份，就是说每个shard都有一个primary shard，负责写入数据，但是还有几个replica shard。primary shard写入数据之后，会将数据同步到其他几个replica shard上去。</p>

        <p><img src="https://github.com/doocs/advanced-java/raw/main/docs/high-concurrency/images/es-cluster.png" alt="es-cluster" /></p>

        <p>通过这个 replica 的方案，每个 shard 的数据都有多个备份，如果某个机器宕机了，还有别的数据副本在别的机器上呢。保证了高可用性。</p>

        <p>ES 集群多个节点，会自动选举一个节点为 master 节点，这个 master 节点其实就是干一些管理的工作的，比如维护索引元数据、负责切换 primary shard 和 replica shard 身份等。要是 master 节点宕机了，那么会重新选举一个节点为 master 节点。</p>

        <p>如果是非 master 节点宕机了，那么会由 master 节点，让那个宕机节点上的 primary shard 的身份转移到其他机器上的 replica shard。接着你要是修复了那个宕机机器，重启了之后，master 节点会控制将缺失的 replica shard 分配过去，同步后续修改的数据之类的，让集群恢复正常。</p>

        <p>说得更简单一点，就是说如果某个非 master 节点宕机了。那么此节点上的 primary shard 不就没了。那好，master 会让 primary shard 对应的 replica shard（在其他机器上）切换为 primary shard。如果宕机的机器修复了，修复后的节点也不再是 primary shard，而是 replica shard。</p>
      </li>
    </ul>
  </li>
  <li>
    <p>ES写入/查询数据的工作原理</p>

    <ul>
      <li>ES写数据过程：
        <ul>
          <li>
            <p>客户端选择一个 node 发送请求过去，这个 node 就是coordinating node（协调节点）。</p>
          </li>
          <li>
            <p>coordinating node对document进行路由，将请求转发给对应的 node（有 primary shard）。</p>
          </li>
          <li>
            <p>实际的 node 上的 primary shard处理请求，然后将数据同步到replica node。</p>
          </li>
          <li>
            <p>coordinating node 如果发现primary node和所有replica node都搞定之后，就返回响应结果给客户端。</p>

            <p><img src="https://github.com/doocs/advanced-java/raw/main/docs/high-concurrency/images/es-write.png" alt="es-write" /></p>
          </li>
        </ul>
      </li>
      <li>
        <p>ES读数据过程</p>

        <p>可以通过doc id来查询，会根据doc id进行 hash，判断出来当时把doc id分配到了哪个shard上面去，从那个 shard 去查询。</p>

        <ul>
          <li>客户端发送请求到任意一个 node，成为coordinate node。</li>
          <li>coordinate node对doc id进行哈希路由，将请求转发到对应的 node，此时会使用round-robin<strong>随机轮询算法</strong>，在primary shard以及其所有 replica 中随机选择一个，让读请求负载均衡。</li>
          <li>接收请求的 node 返回 document 给coordinate node。</li>
          <li>coordinate node返回 document 给客户端。</li>
        </ul>
      </li>
      <li>
        <p>ES搜索数据过程</p>

        <p>ES可以做到全文搜索，即先建立索引，在对索引进行搜索。用户可以根据关键词来搜索，将包含该关键词的 <code class="language-plaintext highlighter-rouge">document</code> 给搜索出来</p>

        <ul>
          <li>客户端发送请求到一个coordinate node。</li>
          <li>协调节点将搜索请求转发到<strong>所有</strong>的 shard 对应的primary shard或replica shard，都可以。</li>
          <li>query phase：每个 shard 将自己的搜索结果（其实就是一些doc id）返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。</li>
          <li>fetch phase：接着由协调节点根据doc id去各个节点上<strong>拉取实际</strong>的document数据，最终返回给客户端。</li>
        </ul>

        <p>PS:写请求是写入 primary shard，然后同步给所有的 replica shard；读请求可以从 primary shard 或 replica shard 读取，采用的是随机轮询算法。</p>
      </li>
    </ul>
  </li>
  <li>
    <p>ES写数据底层原理：</p>

    <p><img src="https://github.com/doocs/advanced-java/raw/main/docs/high-concurrency/images/es-write-detail.png" alt="es-write-detail" /></p>

    <ul>
      <li>
        <p>先写入内存 buffer，在 buffer 里的时候数据是搜索不到的；同时将数据写入 translog 日志文件。如果 buffer 快满了，或者到一定时间，就会将内存 buffer 数据refresh到一个新的segment file中，但是此时数据不是直接进入segment file磁盘文件，而是先进入os cache。这个过程就是refresh 。</p>
      </li>
      <li>
        <p>每隔 1 秒钟，es 将 buffer 中的数据写入一个<strong>新的</strong>segment file，每秒钟会产生一个<strong>新的磁盘文件</strong>segment file，这个segment file中就存储最近 1 秒内 buffer 中写入的数据。但是如果 buffer 里面此时没有数据，那当然不会执行 refresh 操作，如果 buffer 里面有数据，默认 1 秒钟执行一次 refresh 操作，刷入一个新的 segment file 中。</p>

        <p><strong>PS</strong>:操作系统里面，磁盘文件其实都有一个东西，叫做 os cache，即操作系统缓存，就是说数据写入磁盘文件之前，会先进入os cache，先进入操作系统级别的一个内存缓存中去。只要 <code class="language-plaintext highlighter-rouge">buffer</code> 中的数据被 refresh 操作刷入os cache中，这个数据就可以被搜索到了。</p>

        <p>为什么叫 es 是<strong>准实时</strong>的？ <code class="language-plaintext highlighter-rouge">NRT</code> ，全称 near real-time。默认是每隔 1 秒 refresh 一次的，所以 es 是准实时的，因为写入的数据 1 秒之后才能被看到。可以通过 es 的 restful api或者 java api ，<strong>手动</strong>执行一次 refresh 操作，就是手动将 buffer 中的数据刷入 <code class="language-plaintext highlighter-rouge">os cache</code> 中，让数据立马就可以被搜索到。只要数据被输入 os cache中，buffer 就会被清空了，因为不需要保留 buffer 了，数据在 translog 里面已经持久化到磁盘去一份了。</p>
      </li>
      <li>
        <p>重复上面的步骤，新的数据不断进入 buffer 和 translog，不断将buffer数据写入一个又一个新的segment file中去，每次refresh完 buffer 清空，translog 保留。随着这个过程推进，translog 会变得越来越大。当 translog 达到一定长度的时候，就会触发commit操作。</p>
      </li>
      <li>
        <p>commit 操作发生第一步，就是将 buffer 中现有数据 <code class="language-plaintext highlighter-rouge">refresh</code> 到 <code class="language-plaintext highlighter-rouge">os cache</code> 中去，清空 buffer。然后，将一个 <code class="language-plaintext highlighter-rouge">commit point</code> 写入磁盘文件，里面标识着这个 <code class="language-plaintext highlighter-rouge">commit point</code> 对应的所有 <code class="language-plaintext highlighter-rouge">segment file</code> ，同时强行将 <code class="language-plaintext highlighter-rouge">os cache</code> 中目前所有的数据都 <code class="language-plaintext highlighter-rouge">fsync</code> 到磁盘文件中去。最后<strong>清空</strong> 现有 translog 日志文件，重启一个 translog，此时 commit 操作完成。</p>

        <p>这个 commit 操作叫做flush。默认 30 分钟自动执行一次flush，但如果 translog 过大，也会触发flush。flush 操作就对应着 commit 的全过程，我们可以通过 es api，手动执行 flush 操作，手动将 os cache 中的数据 fsync 强刷到磁盘上去。</p>
      </li>
      <li>
        <p>translog 日志文件的作用是什么？你执行 commit 操作之前，数据要么是停留在 buffer 中，要么是停留在 os cache 中，无论是 buffer 还是 os cache 都是内存，一旦这台机器死了，内存中的数据就全丢了。所以需要将数据对应的操作写入一个专门的日志文件 <code class="language-plaintext highlighter-rouge">translog</code> 中，一旦此时机器宕机，再次重启的时候，es 会自动读取 translog 日志文件中的数据，恢复到内存 buffer 和 os cache 中去。</p>

        <p>translog 其实也是先写入 os cache 的，默认每隔 5 秒刷一次到磁盘中去，所以默认情况下，可能有 5 秒的数据会仅仅停留在 buffer 或者 translog 文件的 os cache 中，如果此时机器挂了，会<strong>丢失</strong> 5 秒钟的数据。但是这样性能比较好，最多丢 5 秒的数据。也可以将 translog 设置成每次写操作必须是直接 <code class="language-plaintext highlighter-rouge">fsync</code> 到磁盘，但是性能会差很多。</p>

        <ul>
          <li><code class="language-plaintext highlighter-rouge">index.translog.sync_interval</code> 控制 translog 多久 fsync 到磁盘,最小为 100ms；</li>
          <li><code class="language-plaintext highlighter-rouge">index.translog.durability</code> translog 是每 5 秒钟刷新一次还是每次请求都 fsync，这个参数有 2 个取值：request(每次请求都执行 fsync,es 要等 translog fsync 到磁盘后才会返回成功)和 async(默认值，translog 每隔 5 秒钟 fsync 一次)。</li>
        </ul>

        <p>实际上你在这里，如果面试官没有问你 es 丢数据的问题，你可以在这里给面试官炫一把，你说，其实 es 第一是准实时的，数据写入 1 秒后可以搜索到；可能会丢失数据的。有 5 秒的数据，停留在 buffer、translog os cache、segment file os cache 中，而不在磁盘上，此时如果宕机，会导致 5 秒的<strong>数据丢失</strong>。</p>
      </li>
      <li>
        <p><strong>总结</strong>：数据先写入内存 buffer，然后每隔 1s，将数据 refresh 到 os cache，到了 os cache 数据就能被搜索到（所以我们才说 es 从写入到能被搜索到，中间有 1s 的延迟）。每隔 5s，将数据写入 translog 文件（这样如果机器宕机，内存数据全没，最多会有 5s 的数据丢失），translog 大到一定程度，或者默认每隔 30mins，会触发 commit 操作，将缓冲区的数据都 flush 到 segment file 磁盘文件中。</p>

        <p>数据写入 segment file 之后，同时就建立好了倒排索引。</p>
      </li>
    </ul>
  </li>
  <li>
    <p>删除/更新数据底层原理</p>

    <ul>
      <li>如果是删除操作，commit 的时候会生成一个 <code class="language-plaintext highlighter-rouge">.del</code> 文件，里面将某个 doc 标识为 <code class="language-plaintext highlighter-rouge">deleted</code> 状态，那么搜索的时候根据 <code class="language-plaintext highlighter-rouge">.del</code> 文件就知道这个 doc 是否被删除了。</li>
      <li>如果是更新操作，就是将原来的 doc 标识为 <code class="language-plaintext highlighter-rouge">deleted</code> 状态，然后新写入一条数据。</li>
      <li>buffer 每 refresh 一次，就会产生一个segment file，所以默认情况下是 1 秒钟一个segment file，这样下来segment file会越来越多，此时会定期执行 merge。每次 merge 的时候，会将多个segment file合并成一个，同时这里会将标识为deleted的 doc 给<strong>物理删除掉</strong>，然后将新的segment file写入磁盘，这里会写一个commit point，标识所有新的 segment file，然后打开segment file供搜索使用，同时删除旧的segment file。</li>
    </ul>
  </li>
  <li>
    <p>底层Iucene</p>

    <p>简单来说，lucene 就是一个 jar 包，里面包含了封装好的各种建立倒排索引的算法代码。我们用 Java 开发的时候，引入 lucene jar，然后基于 lucene 的 api 去开发就可以了。</p>

    <p>通过 lucene，我们可以将已有的数据建立索引，lucene 会在本地磁盘上面，给我们组织索引的数据结构。</p>
  </li>
  <li>
    <p>倒排索引</p>

    <p>在搜索引擎中，每个文档都有一个对应的文档 ID，文档内容被表示为一系列关键词的集合。例如，文档 1 经过分词，提取了 20 个关键词，每个关键词都会记录它在文档中出现的次数和出现位置。</p>

    <p>那么，倒排索引就是<strong>关键词到文档</strong> ID 的映射，每个关键词都对应着一系列的文件，这些文件中都出现了关键词。</p>

    <p>另外，实用的倒排索引还可以记录更多的信息，比如文档频率信息，表示在文档集合中有多少个文档包含某个单词。</p>

    <p>那么，有了倒排索引，搜索引擎可以很方便地响应用户的查询。比如用户输入查询 <code class="language-plaintext highlighter-rouge">Facebook</code> ，搜索系统查找倒排索引，从中读出包含这个单词的文档，这些文档就是提供给用户的搜索结果。</p>

    <p>要注意倒排索引的两个重要细节：</p>

    <ul>
      <li>倒排索引中的所有词项对应一个或多个文档；</li>
      <li>倒排索引中的词项根据字典顺序升序排列</li>
    </ul>
  </li>
</ul>

  </article>

</div>

    </div>

  </body>

</html>
